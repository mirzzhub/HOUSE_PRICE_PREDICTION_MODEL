# -*- coding: utf-8 -*-
"""Linear Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-l_5RQCNnE5lF29K1YPNeeV_6ySzgHHR
"""

import pandas as pd

#importing the datasets
data = pd.read_csv('/content/train.csv')
test = pd.read_csv('/content/test.csv')
data.head()

data.info()
#summary of data types, non null entries and memory usage

#summary statistics like mean, SD , min, max to understand the distribution of the data
data.describe()

# to Visualize feature distributions (consider using histograms or boxplots)
data.plot.hist(subplots=True,layout=(10,5) ,figsize=(20,20))
plt.show()

# Explore outliers with boxplots (consider features that might have outliers)
data[['SalePrice','LotArea','GrLivArea']].plot(kind='box')
plt.show()

#to identify the count of missing values in each column of trainig dataset
missing_value = (data.isnull().sum())
print(missing_value[missing_value > 0])

#to identify the count of missing values in each column of testing dataset
missing_value_1 = (test.isnull().sum())
print(missing_value_1[missing_value_1 > 0])

#drop the rows with missing data
#  data.dropna()

data.dtypes
#to know the data types of the df

categorical_features = [data.columns]
categorical_features
# to view the features in the df by catogory

# Printing out all features
features = data.columns
print("Features in the dataset:")
for feature in features:
    print(feature)

data.shape
 #to find the size of the data set

import seaborn as sns;
import matplotlib.pyplot as plt

sns.histplot(data.SalePrice,bins=50);

#to visualize the distribution of sales price vs the count of houses

# features = data.columns (to include all the columns)

#take only the necessary columns
features = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'TotRmsAbvGrd']

#Assign the data
x = data[features] #independent variable
y = data.SalePrice #dependent variable ie. need to be predicted

x.head()

#Splitting the data into train and testing

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 50)

# test_size = 0.2 means 20% of data is used for testing

#now fitting the model for linear regression
from sklearn.linear_model import LinearRegression
model = LinearRegression()

#TRAINING
#then set the training data sets into the model
model.fit(x_train, y_train)

#TESTING
# To make prediction on testing data
y_pred = model.predict(x_test)
model.score(x_test, y_test)

"""WHAT THE OUTPUT SAYS :

model.score(x_test, y_test)

This line evaluates the performance of the model on the testing data. The output (0.7393 in your case) represents the R-squared score, which is a common metric for linear regression. An R-squared of 0.74 indicates that the model explains 75% of the variance in the target variable.

An R-squared score of 0.75 indicates that your model explains 75% of the variation in the target variable (house sale prices, based on your previous code examples)  using the features in your training data. In other words, 75% of the differences observed in sale prices can be explained by the relationships between those prices and the features you included in your model.

The R-squared doesn't directly translate to the percentage of identical predictions between your model's output and the actual values.

It indicates the proportion of variance explained by the model.

VARIANCE : SPREAD OF DATA

It doesn't necessarily mean 75% of the actual house prices fall exactly within a certain range around the average price

The model can explain 75% of the overall "spread" in house prices based on the features you included. The remaining 25% of the variance is due to other factors not considered by the model or random error.

 a model's ability to "explain" is about its capacity to learn patterns and use them to make predictions, even if it can't provide a human-like explanation for those patterns.
"""

# to evaluate the model mean squared error is used

from sklearn.metrics import mean_squared_error, r2_score
mse = mean_squared_error(y_test, y_pred)
print("Mean Squared Error : "+ str(mse))

r2 = r2_score(y_test, y_pred)
print("R2 Score :" +str(r2))

# Mean Squared Error : 1609585449.0576904
# R2 Score :0.7526447721096934

# Predict the price of a new house
import numpy as np

new_house = np.array([[5200, 4, 2,1,1,2,3,1]])
predicted_price = model.predict(new_house)
print(f"Predicted Price for the New House: ${predicted_price[0]:.2f}")

# Plot graph actual vs. predicted values

plt.scatter(y_test, y_pred, c = "green")
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.title('Actual vs Predicted Prices (Linear Regression)')
plt.show()

"""SUMMARY

# Importing libraries


*   import pandas as pd
*   from sklearn.model_selection import train_test_split
*   from sklearn.linear_model import LinearRegression
*   from sklearn.metrics import mean_squared_error, r2_score
*   import seaborn as sns
*   import matplotlib.pyplot as plt

#Importing the datasets

#Summary of data types, non null entries and memory usage
  data.info()

#Summary statistics like mean, SD , min, max to understand the distribution of the data
data.describe()

# To Visualize feature distributions
# Explore outliers with boxplots
# Handle missing values
# To visualize the distribution of sales price vs the count of houses
# Split the train and test data
# Fit the model
# Train
# Test
# Evaluate
# Visualize



"""